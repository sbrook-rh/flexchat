{
  "$schema": "../schema/config-schema.json",
  "comment": "Advanced configuration with multiple strategies: dynamic RAG, LLM intent detection, and fallback",
  
  "providers": {
    "ollama": {
      "type": "ollama",
      "baseUrl": "http://localhost:11434",
      "models": {
        "chat": "qwen2.5:7b-instruct",
        "fast": "qwen2.5:3b-instruct"
      },
      "embedding_model": "nomic-embed-text",
      "comment": [
        "Using Ollama for all AI operations (chat + embeddings).",
        "qwen2.5:7b-instruct for quality responses, qwen2.5:3b-instruct for simple/off-topic",
        "Install: ollama pull qwen2.5:7b-instruct && ollama pull qwen2.5:3b-instruct && ollama pull nomic-embed-text"
      ]
    }
  },
  
  "knowledge_bases": {
    "dynamic": {
      "type": "chromadb-wrapper",
      "url": "http://localhost:5006",
      "embedding_provider": "ollama",
      "comment": [
        "Dynamic collections managed via UI. Users can create collections for different topics",
        "(e.g., openshift-ai, ansible-docs, rhel-admin) with custom metadata per collection."
      ]
    }
  },
  
  "detection_provider": {
    "provider": "ollama",
    "model": "qwen2.5:7b-instruct",
    "comment": "Single LLM for all intent detection to keep responses consistent"
  },
  
  "strategies": [
    {
      "name": "DYNAMIC_RAG",
      "comment": [
        "Strategy for dynamically created knowledge base collections.",
        "Each collection stores its own system_prompt, thresholds, and response settings.",
        "This strategy provides sensible defaults that collections can override."
      ],
      "detection": {
        "type": "rag",
        "knowledge_base": "dynamic",
        "threshold": 0.2,
        "fallback_threshold": 0.5,
        "comment": [
          "Low threshold (0.2) for direct match - high confidence",
          "Fallback threshold (0.5) catches 'maybe relevant' - LLM decides",
          "Individual collections can set their own thresholds in metadata"
        ]
      },
      "response": {
        "provider": "ollama",
        "model": "qwen2.5:7b-instruct",
        "system_prompt": "You are an expert assistant. Use the provided context to answer accurately. If the context doesn't contain the answer, say so.",
        "max_tokens": 1000,
        "temperature": 0.7,
        "comment": "Collections override this with their own specialized prompts"
      }
    },
    {
      "name": "SUPPORT",
      "comment": [
        "LLM-detected strategy for general Red Hat support questions.",
        "No RAG - pure LLM response for troubleshooting, 'how-to', etc."
      ],
      "detection": {
        "type": "llm",
        "description": "if the query is asking for help, troubleshooting, support, or how to do something",
        "comment": "LLM evaluates if this matches during detection phase"
      },
      "response": {
        "provider": "ollama",
        "model": "qwen2.5:7b-instruct",
        "system_prompt": "You are a helpful Red Hat support assistant. Provide practical, step-by-step guidance for technical questions. If you're not certain, recommend checking official documentation.",
        "max_tokens": 1200,
        "temperature": 0.8,
        "comment": "Higher temperature for creative problem-solving"
      }
    },
    {
      "name": "GENERAL",
      "comment": [
        "Fallback for everything else: greetings, off-topic, small talk.",
        "Uses smaller/faster model since these don't need deep knowledge."
      ],
      "detection": {
        "type": "default",
        "comment": "Catches anything not matched by RAG or LLM strategies"
      },
      "response": {
        "provider": "ollama",
        "model": "qwen2.5:3b-instruct",
        "system_prompt": "You are a friendly Red Hat assistant. This query is outside your main expertise. Respond briefly and politely, and suggest asking about Red Hat products, technologies, or support topics.",
        "max_tokens": 100,
        "temperature": 0.7,
        "comment": "Short, polite responses with smaller model to save resources"
      }
    }
  ],
  
  "_example_flow": {
    "comment": [
      "Detection flow example:",
      "1. User asks 'What is InstructLab in OpenShift AI?'",
      "   - Checks selected collections (if any) → finds 'openshift-ai' collection",
      "   - Distance: 0.15 < 0.2 threshold → DIRECT MATCH",
      "   - Uses DYNAMIC_RAG with openshift-ai collection's context",
      "",
      "2. User asks 'How do I deploy to OpenShift?'",
      "   - Checks collections → distance: 0.45 (between 0.2 and 0.5)",
      "   - Adds to candidates, checks other strategies",
      "   - LLM sees: 'openshift-ai' (rag candidate) vs 'SUPPORT' (llm) vs 'GENERAL' (default)",
      "   - LLM picks 'SUPPORT' (how-to question) → uses SUPPORT strategy with no RAG",
      "",
      "3. User asks 'What's the weather?'",
      "   - Checks collections → distance: 0.85 > 0.5 → not a candidate",
      "   - LLM sees: 'SUPPORT' (llm) vs 'GENERAL' (default)",
      "   - LLM picks 'GENERAL' → brief, polite redirect"
    ]
  }
}

