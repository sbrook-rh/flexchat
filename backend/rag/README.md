# ChromaDB Wrapper Service

A Python FastAPI service that provides a **storage-only** HTTP interface to ChromaDB for the FlexChat application.

## Overview

The wrapper service is a **pure storage layer** for ChromaDB, enabling:
- **Pre-computed embeddings storage**: Documents must include pre-computed embedding vectors
- **Dynamic collection management**: Create and manage collections via API
- **Persistent storage**: ChromaDB data stored locally in `chroma_db/`
- **Collection metadata**: Store embedding configuration and custom settings per collection
- **Similarity search**: Query collections using pre-computed embeddings

### Architecture

The wrapper service handles:
- **Document storage**: Validates and stores documents with pre-computed embedding vectors
- **Similarity search**: Queries collections using pre-computed query embeddings
- **Collection management**: CRUD operations for collections with metadata storage
- **Validation**: Ensures embedding format and dimensional consistency

## Quick Start

### 1. Install Dependencies

```bash
cd backend/rag
pip install -r requirements.txt
```

### 2. Configure Environment

Copy `env.example` to `.env` and configure your embedding provider:

```bash
cp env.example .env
```

**Note**: These settings configure query-time embedding generation. Document embeddings are generated by the Node.js backend.

**For Ollama (local):**
```bash
EMBEDDING_PROVIDER=ollama
EMBEDDING_MODEL=nomic-embed-text
OLLAMA_BASE_URL=http://localhost:11434
```

**For Gemini:**
```bash
EMBEDDING_PROVIDER=gemini
EMBEDDING_MODEL=text-embedding-004
GEMINI_API_KEY=your_api_key_here
```

**For OpenAI:**
```bash
EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL=text-embedding-3-small
OPENAI_API_KEY=your_api_key_here
```

### 3. Start the Service

```bash
python server.py
```

The service will start on `http://localhost:5006`.

Or use the project-wide start script from the root:
```bash
./start.sh
```

## API Endpoints

### Collection Management

#### List Collections
```bash
GET /collections
```

Returns all collections with their document counts and metadata.

#### Get Collection Info
```bash
GET /collections/{name}
```

Returns details about a specific collection including metadata.

#### Create Collection
```bash
POST /collections
Content-Type: application/json

{
  "name": "my-collection",
  "metadata": {
    "embedding_provider": "ollama",
    "embedding_model": "nomic-embed-text:latest",
    "embedding_dimensions": 768,
    "embedding_connection_id": "ollama-local",
    "description": "Description for LLM detection",
    "match_threshold": 0.3,
    "partial_threshold": 0.5,
    "created_at": "2025-01-01T00:00:00.000Z"
  },
  "embedding_provider": "ollama",           // Top-level (Node sends this)
  "embedding_model": "nomic-embed-text:latest"  // Top-level (Node sends this)
}
```

**Notes**:
- Embedding metadata is stored in collection metadata without validation
- Top-level `embedding_provider` and `embedding_model` fields are stored as-is
- Collection metadata is treated as opaque configuration data

#### Delete Collection
```bash
DELETE /collections/{name}
```

### Document Management

#### Add Documents
```bash
POST /collections/{name}/documents
Content-Type: application/json

{
  "documents": [
    {
      "text": "Document content here",
      "metadata": {
        "source": "manual",
        "title": "Document Title"
      },
      "embedding": [0.123, 0.456, ...]  // REQUIRED: pre-computed embedding array
    }
  ]
}
```

**Requirements**:
- **REQUIRED**: Every document MUST include `embedding` field
- Embeddings must be arrays of numbers (validated for type and consistency)
- All embeddings in a batch must have the same dimensions
- Wrapper validates embeddings but does not generate them

### Query

#### Search Collection
```bash
POST /query
Content-Type: application/json

{
  "query": "What is InstructLab?",
  "collection": "openshift-ai",
  "top_k": 3
}
```

Returns relevant documents with distance scores.

## Embedding Configuration

### Document Embeddings
- **Generated by**: Node.js backend
- **Configuration**: LLM connections in FlexChat config
- **Wrapper role**: Validates pre-computed embeddings

### Query Embeddings
- **Generated by**: Python wrapper service
- **Configuration**: Python `.env` file
- **Wrapper role**: Generates embeddings for query requests

### Collection Metadata
Each collection stores complete embedding configuration:
- `embedding_provider`: Provider type (e.g., "ollama", "openai")
- `embedding_model`: Exact model ID (e.g., "nomic-embed-text:latest")
- `embedding_dimensions`: Embedding vector size (e.g., 768, 1536)
- `embedding_connection_id`: FlexChat LLM connection identifier

**Important**: Embedding metadata is immutable after collection creation.

## Storage

- **Database**: `chroma_db/` directory (persistent, excluded from git)
- **Distance metric**: Cosine distance (optimal for normalized text embeddings)
- **Metadata**: Stored per-collection including embedding provider/model

## Integration

The wrapper is designed to work with the Node.js chat server's `ChromaDBWrapperProvider`:

```javascript
// config/config.json
{
  "knowledge_bases": {
    "dynamic": {
      "type": "chromadb-wrapper",
      "url": "http://localhost:5006",
      "embedding_provider": "ollama"
    }
  }
}
```

See `config/examples/rag-wrapper-ollama.json` for complete configuration examples.

## Files

- **`server.py`** - Main FastAPI service
- **`requirements.txt`** - Python dependencies
- **`env.example`** - Environment variable template
- **`chroma_db/`** - ChromaDB persistent storage (created automatically)
- **`.python-version`** - Pyenv version file (if using pyenv)

## Troubleshooting

### "All documents must include pre-computed embeddings"
**Cause**: Document upload without embeddings  
**Solution**: Ensure Node backend generates embeddings before sending to wrapper. Check `embedding-generator.js` logs.

### "Invalid embedding format" or "Dimension mismatch"
**Cause**: Embedding validation failed  
**Solution**: 
- Verify embeddings are arrays of numbers
- All embeddings in batch must have same dimensions
- Check Node backend is using correct model

### "Collection not found" errors
- The collection may not exist yet - create it via FlexChat UI or Node API
- Check that the collection name matches exactly (case-sensitive)

### Query embedding errors
- Verify your API key is set correctly in `.env`
- For Ollama: ensure the model is pulled (`ollama pull nomic-embed-text`)
- Check that wrapper's `.env` provider matches collection metadata

### Distance scores seem wrong
- Collections created with different embedding models are incompatible
- Verify collection metadata has correct `embedding_model` (including version tags)
- Check Node backend is using the exact model specified in collection metadata

### Legacy collections without proper metadata
**Solution**: See `docs/PHASE_05_MIGRATION.md` for metadata update procedures.

For more troubleshooting, see the main project `TROUBLESHOOTING.md` and `docs/PHASE_05_MIGRATION.md`.

## Development

The service is built with:
- **FastAPI**: Modern Python web framework
- **ChromaDB**: Vector database for embeddings
- **Multiple providers**: OpenAI, Gemini, Ollama support

For detailed architecture and implementation notes, see `docs/DYNAMIC_COLLECTIONS_IMPLEMENTATION.md`.
